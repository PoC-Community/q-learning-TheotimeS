{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "### Introduction\n",
    "Today, you'll learn about QLearning using the gym api.\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments.\n",
    "\n",
    "Reinforcement learning is a subset of Artificial Intelligence. Our AI will learn by playing the same game over and over again until it learns what it must do in order to win.\n",
    "\n",
    "In this workshop, you will:\n",
    "\n",
    "1. Learn about Q Tables\n",
    "2. Setup a gym environment\n",
    "3. Train an AI to solve the [FrozenLake environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "Let's start by importing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of Q-Learning: \n",
    "\n",
    "![Example of a Q-Learning Environment](./images/example.png)\n",
    "\n",
    "A game where the AI must get to the ‚ÄúEnd‚Äù coordinates on the map in the shortest amount of time while avoiding the bombs and collecting the bonuses.\n",
    "\n",
    "A Q-Table for this environment could be visualized like this. \n",
    "\n",
    "![Example of a Q-Table](./images/qtable.png)\n",
    "\n",
    "With all of the possible actions listed horizontally and all of the possible states listed vertically.\n",
    "\n",
    "Write a function which generates an empty array of shape `x` * `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initializing an empty table\n",
    "\n",
    "def init_q_table(x: int, y: int) -> np.ndarray:\n",
    "    return np.zeros((x, y))\n",
    "\n",
    "qTable = init_q_table(5, 4)\n",
    "\n",
    "print(\"Q-Table:\\n\" + str(qTable))\n",
    "\n",
    "assert(np.mean(qTable) == 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, the AI receives a **reward** each time it **acts**:\n",
    "\n",
    "This reward is a number that will be interpreted by our AI:\n",
    "\n",
    "- If it is negative, the AI will learn that the action it made which led to this reward was probably not the best\n",
    "- If it is null or positive, the AI will learn that the action it made which led to this reward was probably smart and it should do it again.\n",
    "\n",
    "Here, the AI would likely receive a **negative reward** (-1) if it chooses to move to the right (because there's a bomb)\n",
    "\n",
    "However, if it went anywhere else, it would probably receive a **neutral reward** (0), since it would end up on blank squares.\n",
    "\n",
    "Assuming it decided to go to the right, the AI would encounter a bomb and the value for [Start, Move_Right] would decrease.\n",
    "\n",
    "\n",
    "Once we receive a reward, we can use the Q Formula below to update our Q Table's values. Our AI will use these values to learn what the most profitable action is at each state of the environment.\n",
    "\n",
    "![QFormula](./images/qformula.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation:\n",
    "\n",
    "**Q<sup>new</sup>(s<sub>t</sub>, a<sub>t</sub>)** = the new value for the current State and Action on our Qtable\n",
    "\n",
    "**Q(s<sub>t</sub>, a<sub>t</sub>)** = the old value for the current State and Action on our QTable\n",
    "\n",
    "**Œ± (learning rate)** = a constant value we use for our algorithm's learning speed (usually a number between 0.5 and 0.05)\n",
    "\n",
    "**r<sub>t</sub> (reward)** = the reward received (as per the example above, -1 if you hit a bomb)\n",
    "\n",
    "**ùõæ (discount factor)** = another constant value we use to determine how good of a memory our AI has\n",
    "\n",
    "**max Q<sup>new</sup>(s<sub>t+1</sub>, a)** = the value of the most profitable action at the new state\n",
    "\n",
    "##### Now that you know how a Q Table updates itself using the Q Formula, let's try to implement this formula as a python function:\n",
    "(You can use the `LEARNING_RATE` and `DISCOUNT_RATE` constants in your formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "DISCOUNT_RATE = 0.99\n",
    "\n",
    "def q_function(q_table: np.ndarray, state: int, action: int, reward: int, newState: int) -> float:\n",
    "    q_table[state][action] = q_table[state][action] + LEARNING_RATE * (reward + DISCOUNT_RATE * np.max(q_table[newState]) - q_table[state][action])\n",
    "    return q_table[state][action]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can test the Q Function to see if it works !\\\n",
    "If qTable[0, 1]'s value below is `-0.05`, congrats: you have successfully implemented the formula !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table after action:\n",
      "[[ 0.   -0.05  0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "q_table = init_q_table(5,4)\n",
    "\n",
    "q_table[0, 1] = q_function(q_table, state=0, action=1, reward=-1, newState=3)\n",
    "\n",
    "print(\"Q-Table after action:\\n\" + str(q_table))\n",
    "\n",
    "assert(q_table[0, 1] == -LEARNING_RATE), f\"The Q function is incorrect: the value of qTable[0, 1] should be -{LEARNING_RATE}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up the GYM Environment\n",
    "\n",
    "#### What is Gym ?\n",
    "> Gym is a standard API for reinforcement learning, and a diverse collection of reference environments\n",
    ">\n",
    "> -- <cite>Gym Docs</cite>\n",
    "\n",
    "Let's get to the fun part:\n",
    "Read the documentation for FrozenLake [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "and find out how to load the environment.\n",
    "\n",
    "For now, we will set the `is_slippery` variable to `False` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some code to load and make the FrozenLake environment:\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our environment is loaded, we need a function that performs a random action.\n",
    "\n",
    "An action is a number between 0 and the total number of possible actions.\n",
    "\n",
    "Try to find the value (inside `env`) that gives this total number of actions in the environment and store it in `total_actions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_actions = env.action_space.n\n",
    "\n",
    "assert(total_actions == 4), f\"There are a total of four possible actions in this environment. Your answer is {total_actions}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use `total_actions` to make a function which returns a random action each time it is called !\n",
    "\n",
    "There is also another way to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action(env):\n",
    "    return random.randint(0, env.action_space.n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `random_action` function, you can run the code below which will display the first frame of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tschmeltz/.local/lib/python3.9/site-packages/gymnasium/envs/toy_text/frozen_lake.py:328: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tschmeltz/q-learning-TheotimeS/Q_Learning.ipynb Cell 18\u001b[0m line \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tschmeltz/q-learning-TheotimeS/Q_Learning.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m observation, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tschmeltz/q-learning-TheotimeS/Q_Learning.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Displaying the first frame of the game\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tschmeltz/q-learning-TheotimeS/Q_Learning.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(env\u001b[39m.\u001b[39;49mrender())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tschmeltz/q-learning-TheotimeS/Q_Learning.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Printing game info\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tschmeltz/q-learning-TheotimeS/Q_Learning.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactions: \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mstates: \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mn\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/site-packages/matplotlib/pyplot.py:2903\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2897\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2898\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2899\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2900\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[1;32m   2901\u001b[0m         filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m, resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2902\u001b[0m         data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2903\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2904\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2905\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2906\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2907\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2908\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2909\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2910\u001b[0m     sci(__ret)\n\u001b[1;32m   2911\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m/usr/lib64/python3.9/site-packages/matplotlib/__init__.py:1364\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1363\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1364\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1366\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1367\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1368\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/usr/lib64/python3.9/site-packages/matplotlib/axes/_axes.py:5609\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5604\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5605\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation, origin, extent,\n\u001b[1;32m   5606\u001b[0m                       filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad,\n\u001b[1;32m   5607\u001b[0m                       resample\u001b[39m=\u001b[39mresample, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5609\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5610\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5611\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5612\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.9/site-packages/matplotlib/image.py:700\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39msafe_masked_invalid(A, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    698\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[1;32m    699\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[0;32m--> 700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m    703\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    704\u001b[0m     \u001b[39m# If just one dimension assume scalar and apply colormap\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMX0lEQVR4nO3bX4il9X3H8fenuxEak0aJk5DuKt2WNbotWnRiJPSPaWizay6WgBdqqFQCixBDLpVCk4I3zUUhBP8siyySm+xNJN0UEyktiQVr4yz4bxVlulKdrOAaQwoGKqvfXsxpc3q+szvPrGfO2cH3CwbmeZ7fOefLMOc9zzzzTKoKSRr3G/MeQNL5xzBIagyDpMYwSGoMg6TGMEhq1g1DksNJXk/y3BmOJ8m3kywneSbJNdMfU9IsDTljeAjYe5bj+4Ddo48DwAPvfSxJ87RuGKrqMeDNsyzZD3ynVj0BXJTkE9MaUNLsbZ/Cc+wAXh3bXhnte21yYZIDrJ5VcOGFF157xRVXTOHlJZ3JsWPH3qiqhY0+bhphyBr71rzPuqoOAYcAFhcXa2lpaQovL+lMkvznuTxuGn+VWAEuHdveCZycwvNKmpNphOEocNvorxPXA7+sqvZrhKStY91fJZJ8F7gBuCTJCvAN4AMAVXUQeAS4EVgGfgXcvlnDSpqNdcNQVbesc7yAr0xtIklz552PkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySmkFhSLI3yYtJlpPcvcbxjyT5QZKnkxxPcvv0R5U0K+uGIck24D5gH7AHuCXJnollXwGer6qrgRuAv09ywZRnlTQjQ84YrgOWq+pEVb0NHAH2T6wp4MNJAnwIeBM4PdVJJc3MkDDsAF4d214Z7Rt3L3AlcBJ4FvhaVb07+URJDiRZSrJ06tSpcxxZ0mYbEoassa8mtj8PPAX8NvCHwL1Jfqs9qOpQVS1W1eLCwsIGR5U0K0PCsAJcOra9k9Uzg3G3Aw/XqmXgZeCK6YwoadaGhOFJYHeSXaMLijcDRyfWvAJ8DiDJx4FPAiemOaik2dm+3oKqOp3kTuBRYBtwuKqOJ7ljdPwgcA/wUJJnWf3V466qemMT55a0idYNA0BVPQI8MrHv4NjnJ4G/mO5okubFOx8lNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVIzKAxJ9iZ5MclykrvPsOaGJE8lOZ7kJ9MdU9IsbV9vQZJtwH3AnwMrwJNJjlbV82NrLgLuB/ZW1StJPrZJ80qagSFnDNcBy1V1oqreBo4A+yfW3Ao8XFWvAFTV69MdU9IsDQnDDuDVse2V0b5xlwMXJ/lxkmNJblvriZIcSLKUZOnUqVPnNrGkTTckDFljX01sbweuBb4AfB74mySXtwdVHaqqxapaXFhY2PCwkmZj3WsMrJ4hXDq2vRM4ucaaN6rqLeCtJI8BVwMvTWVKSTM15IzhSWB3kl1JLgBuBo5OrPkH4I+TbE/yQeDTwAvTHVXSrKx7xlBVp5PcCTwKbAMOV9XxJHeMjh+sqheS/Ah4BngXeLCqntvMwSVtnlRNXi6YjcXFxVpaWprLa0vvF0mOVdXiRh/nnY+SGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJKaQWFIsjfJi0mWk9x9lnWfSvJOkpumN6KkWVs3DEm2AfcB+4A9wC1J9pxh3TeBR6c9pKTZGnLGcB2wXFUnqupt4Aiwf411XwW+B7w+xfkkzcGQMOwAXh3bXhnt+z9JdgBfBA6e7YmSHEiylGTp1KlTG51V0owMCUPW2FcT298C7qqqd872RFV1qKoWq2pxYWFh4IiSZm37gDUrwKVj2zuBkxNrFoEjSQAuAW5Mcrqqvj+NISXN1pAwPAnsTrIL+BlwM3Dr+IKq2vW/nyd5CPhHoyBtXeuGoapOJ7mT1b82bAMOV9XxJHeMjp/1uoKkrWfIGQNV9QjwyMS+NYNQVX/13seSNE/e+SipMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkhrDIKkxDJIawyCpMQySGsMgqTEMkppBYUiyN8mLSZaT3L3G8S8leWb08XiSq6c/qqRZWTcMSbYB9wH7gD3ALUn2TCx7GfjTqroKuAc4NO1BJc3OkDOG64DlqjpRVW8DR4D94wuq6vGq+sVo8wlg53THlDRLQ8KwA3h1bHtltO9Mvgz8cK0DSQ4kWUqydOrUqeFTSpqpIWHIGvtqzYXJZ1kNw11rHa+qQ1W1WFWLCwsLw6eUNFPbB6xZAS4d294JnJxclOQq4EFgX1X9fDrjSZqHIWcMTwK7k+xKcgFwM3B0fEGSy4CHgb+sqpemP6akWVr3jKGqTie5E3gU2AYcrqrjSe4YHT8IfB34KHB/EoDTVbW4eWNL2kypWvNywaZbXFyspaWluby29H6R5Ni5/JD2zkdJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBknNoDAk2ZvkxSTLSe5e43iSfHt0/Jkk10x/VEmzsm4YkmwD7gP2AXuAW5LsmVi2D9g9+jgAPDDlOSXN0JAzhuuA5ao6UVVvA0eA/RNr9gPfqVVPABcl+cSUZ5U0I9sHrNkBvDq2vQJ8esCaHcBr44uSHGD1jALgv5M8t6Fp5+sS4I15DzHQVpoVtta8W2lWgE+ey4OGhCFr7KtzWENVHQIOASRZqqrFAa9/XthK826lWWFrzbuVZoXVec/lcUN+lVgBLh3b3gmcPIc1kraIIWF4EtidZFeSC4CbgaMTa44Ct43+OnE98Muqem3yiSRtDev+KlFVp5PcCTwKbAMOV9XxJHeMjh8EHgFuBJaBXwG3D3jtQ+c89XxspXm30qywtebdSrPCOc6bqnYpQNL7nHc+SmoMg6Rm08OwlW6nHjDrl0YzPpPk8SRXz2POsXnOOu/Yuk8leSfJTbOcb2KGdWdNckOSp5IcT/KTWc84Mct63wsfSfKDJE+P5h1yXW1TJDmc5PUz3Rd0Tu+xqtq0D1YvVv4H8LvABcDTwJ6JNTcCP2T1XojrgX/fzJne46yfAS4efb5vXrMOnXds3b+weoH4pvN1VuAi4HngstH2x87nry3w18A3R58vAG8CF8xp3j8BrgGeO8PxDb/HNvuMYSvdTr3urFX1eFX9YrT5BKv3a8zLkK8twFeB7wGvz3K4CUNmvRV4uKpeAaiq833eAj6cJMCHWA3D6dmOORqk6rHR65/Jht9jmx2GM90qvdE1s7DROb7MaoXnZd15k+wAvggcnOFcaxnytb0cuDjJj5McS3LbzKbrhsx7L3AlqzfyPQt8rarenc14G7bh99iQW6Lfi6ndTj0Dg+dI8llWw/BHmzrR2Q2Z91vAXVX1zuoPtrkZMut24Frgc8BvAv+W5Imqemmzh1vDkHk/DzwF/Bnwe8A/JfnXqvqvTZ7tXGz4PbbZYdhKt1MPmiPJVcCDwL6q+vmMZlvLkHkXgSOjKFwC3JjkdFV9fyYT/trQ74M3quot4K0kjwFXA/MIw5B5bwf+rlZ/iV9O8jJwBfDT2Yy4IRt/j23yRZHtwAlgF7++iPP7E2u+wP+/MPLTOV3AGTLrZaze3fmZecy40Xkn1j/E/C4+DvnaXgn882jtB4HngD84j+d9APjb0ecfB34GXDLH74ff4cwXHzf8HtvUM4bavNup5zXr14GPAvePfgqfrjn9p93Aec8LQ2atqheS/Ah4BngXeLCq5vJv+QO/tvcADyV5ltU33F1VNZd/x07yXeAG4JIkK8A3gA+Mzbrh95i3REtqvPNRUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUvM/YA1djYGMYyEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "# Performing an action\n",
    "action = random_action(env)\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "\n",
    "# Displaying the first frame of the game\n",
    "plt.imshow(env.render())\n",
    "\n",
    "# Printing game info\n",
    "print(f\"actions: {env.action_space.n}\\nstates: {env.observation_space.n}\")\n",
    "print(f\"Current state: {observation}\")\n",
    "\n",
    "# Closing the environment\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, there are **4 possible actions** for each of the **16 possible states**.\\\n",
    "Feel free to play around with the code above to get a better understanding of the API.\n",
    "\n",
    "## 3. Solving the environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a Q Table, Q Function and our environment, we can create our `game_loop`.\n",
    "\n",
    "Call `qFunc()` using the correct arguments it requires in the code below to update the `qTable[,]` using the values available in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_loop(env: gym.Env, q_table: np.ndarray, state: int, action: int) -> tuple:\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see how the environment runs using your `game_loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "state, info = env.reset()\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = random_action(env)\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to see if our AI learns, we will launch the environment 1000 times and see how the Q-Table evolves.\n",
    "\n",
    "Notice how in the code below, we do not call `env.render()` each frame because if we did, our 1000 iterations would take a **lot** of time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 20000\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        # This time, we won't render the game each frame because it would take too long\n",
    "        action = random_action(env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "# Printing the QTable result:\n",
    "for states in q_table:\n",
    "    for actions in states:\n",
    "        if (actions == max(states)):\n",
    "            print(\"\\033[4m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[0m\", end=\"\")\n",
    "        if (actions > 0):\n",
    "            print(\"\\033[92m\", end=\"\")\n",
    "        else:\n",
    "            print(\"\\033[00m\", end=\"\")\n",
    "        print(round(actions, 3), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great ! We now have a nice Q-Table that indicates which action is best for each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it would probably be better for our agent to choose actions based on its experience, rather than at random.\n",
    "Write a function which chooses the best action for each given state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(q_table: np.ndarray, state: int) -> int:\n",
    "    \"\"\"\n",
    "    Write a function which finds the best action for the given state.\n",
    "\n",
    "    It should return its index.\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see the result of our training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "state, info = env.reset()\n",
    "while (True):\n",
    "    env.render()\n",
    "    action = best_action(q_table, state)\n",
    "    q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If all went well, our AI should easily reach its goal !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the AI should probably try to explore all the different possibilities before it starts optimising its gain by following the Q Table...\\\n",
    "In order to solve this problem, we can use the Epsilon-Greedy strategy!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Epsilon Greedy ?\n",
    "\n",
    "Epsilon greedy is a strategy used to make sure that our AI explores its environment and doesn‚Äôt miss out on some cool easter eggs !\n",
    "\n",
    "![EpsilonGreedy](https://steadfast-ragdoll-d83.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa8444925-9940-46b2-b9e3-aae5ca792a74%2FUntitled.png?table=block&id=ce90b632-5d78-4cb0-8101-b326b91bf25d&spaceId=a008bb22-92df-498d-b32c-15988ddb70b9&width=770&userId=&cache=v2)\n",
    "\n",
    "<cite>[source](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870)</cite>\n",
    "\n",
    "```\n",
    "if random(0, 1) > epsilon-greedy:\n",
    "\treturn bestKnownAction\n",
    "else:\n",
    "\treturn randomAction\n",
    "```\n",
    "\t\t\n",
    "The higher the epsilon-greedy value is (from 0 to 1), the higher chance of the AI picking an action at random.\n",
    "\n",
    "The lower the epsilon-greedy value is (from 0 to 1 still), the higher chance of the AI picking an action it considers the most rewarding.\n",
    "\n",
    "In other words, with a high epsilon-greedy value, the robot pictured above might go to the right whereas it would probably choose the left path if it had a lower epsilon-greedy value, therefore missing out on a special discovery !\n",
    "\n",
    "\n",
    "\n",
    "Try to implement it into the following function that we'll use to determine which action the AI will choose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(epsilon: float, q_table: np.ndarray, state: int, env: gym.Env) -> int:\n",
    "    \"\"\"\n",
    "    Write a function to either return a random action or the best action using the functions\n",
    "    defined earlier.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, the AI will explore all possible actions before it chooses the best state, reducing the risk of it missing some golden solutions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we're not done...\\\n",
    "You might have noticed that when we load our environment, we give it a certain argument:\\\n",
    "`is_slippery=False`\n",
    "\n",
    "This argument makes the game far easier !\n",
    "If you want a real challenge, set it to true."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the environment will be much harder to solve if the ice is slippery. Our agent's movements will be unpredictable and it will be impossible to make a 100% accurate AI.\n",
    "\n",
    "Therefore, keep track of the number of times the AI gets to its goal during our testing phase.\n",
    "\n",
    "##### Try to get the highest accuracy possible !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "\n",
    "q_table = init_q_table(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "# Training the AI\n",
    "epsilon = 1.0\n",
    "for i in range(10000):\n",
    "    epsilon = max(epsilon - 0.0001, 0)\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(epsilon, q_table, state, env)\n",
    "        q_table, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            break\n",
    "# Testing the AI\n",
    "wins = 0.0\n",
    "for i in range(100):\n",
    "    state, info = env.reset()\n",
    "    while (True):\n",
    "        action = choose_action(0, q_table, state, env)\n",
    "        _, state, done, reward = game_loop(env, q_table, state, action)\n",
    "        if done:\n",
    "            if (reward > 0):\n",
    "                wins += 1\n",
    "            break\n",
    "\n",
    "print(f\"{round(wins / (i+1) * 100, 2)}% winrate\")\n",
    "print(wins)\n",
    "\n",
    "# Displaying the last frame of the game\n",
    "plt.imshow(env.render())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job ! You completed this workshop !\n",
    "If you want to continue in the wonderful world of reinforcement learning, you could try:\n",
    "- Try and increase the accuracy by using different methods of reinforcement learning.\n",
    "- A custom map for Frozen Lake. For example: `gym.make('FrozenLake-v1', map_name=\"8x8\", is_slippery=True)`\n",
    "- A different environment from https://gymnasium.farama.org/\n",
    "- Going **deeper** with https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "- Trying what you've learned with games you know and love:\n",
    "    - https://gymnasium.farama.org/environments/atari/\n",
    "    - https://pypi.org/project/gym-super-mario-bros/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cba5719d159acad27bb7f37d182d27b1d38df28115fbd0d331121e7aca3605"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
